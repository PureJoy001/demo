import streamlit as st
import pymupdf4llm
import chromadb
from PIL import Image
from util import extract_image_context
from vanna.utils import deterministic_uuid
import base64
import openai
from llama_index.llms.openai import OpenAI as llamaindex_openai
from llama_index.core.chat_engine import CondenseQuestionChatEngine
from chromadb import Documents, EmbeddingFunction, Embeddings
from sentence_transformers import SentenceTransformer
from  openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings, load_index_from_storage, StorageContext
from prompt_templates import custom_prompt
from llama_index.readers.file import PDFReader
import os
try:
  from llama_index import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader, Sto
except ImportError:
  from llama_index.core import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader

st.set_page_config(page_title="mmai chat", page_icon="ðŸ¦™", layout="centered", initial_sidebar_state="auto", menu_items=None)
openai.api_key = "sk-aal58s2gWETYNHTiD49904329c7942C7Ac8dC70625041265"
st.title("Chat with your files ðŸ’¬")
st.info("å›žç­”å°†åŸºäºŽä½ çš„æ–‡ä»¶å†…å®¹", icon="ðŸ“ƒ")

Settings.llm = llamaindex_openai(model="gpt-4o", temperature=0.3, system_prompt="ä½ æ˜¯ä¸€ä½å›žç­”ç”¨æˆ·é—®é¢˜çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œç”±åŒ—é‚®MMAIå®žéªŒå®¤ç ”å‘ã€‚"
                                                                            "å‡è®¾æ‰€æœ‰é—®é¢˜éƒ½ä¸Žå¯¹åº”æ–‡æ¡£èµ„æ–™èµ„æ–™"
                                                                            "æœ‰å…³ã€‚ä¿æŒä½ çš„ç­”æ¡ˆä»¥äº‹å®žä¸ºåŸºç¡€â€”â€”ä¸è¦äº§ç”Ÿå¹»è§‰ã€‚å¦‚æžœç”¨æˆ·çš„é—®é¢˜æ˜¯è¦å¯»æ‰¾æŸäº›å›¾ç‰‡æˆ–ç¤ºæ„å›¾ï¼Œ\
                                                                            æˆ‘ä¼šä»Žå…¶ä»–åœ°æ–¹èŽ·å–å›¾ç‰‡ï¼Œå› æ­¤æ­¤æ—¶ä½ åªéœ€è¦å›žç­”çŸ¥è¯†åº“ä¸­å’Œç”¨æˆ·æé—®æœ‰å…³çš„éƒ¨åˆ†ï¼Œå¹¶åœ¨æœ€åŽåŠ ä¸Šç±»ä¼¼â€œä»¥ä¸‹æ˜¯xxç¤ºæ„å›¾â€çš„å›žç­”ï¼Œ\
                                                                            æˆ‘ä¼šåœ¨ä½ çš„å›žç­”ä¹‹åŽåŠ ä¸Šæˆ‘ä»Žå…¶ä»–æµç¨‹èŽ·å–çš„å›¾ç‰‡ï¼Œä¸è¦å‡ºçŽ°ç±»ä¼¼â€œæ— æ³•æ‰¾åˆ°ç¤ºæ„å›¾â€ä¹‹ç±»çš„å›žç­”ã€‚å¦‚æžœç”¨æˆ·æé—®ä¸æ¶‰åŠ\
                                                                            å¯»æ‰¾å›¾ç‰‡ç›¸å…³çš„æ„å›¾ï¼Œä½ åªéœ€è¦æ­£å¸¸æ ¹æ®çŸ¥è¯†åº“èµ„æ–™å›žç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œç”¨ä¸­æ–‡å›žç­”ã€‚",
                                                                  api_base="https://ai-yyds.com/v1", api_key="sk-aal58s2gWETYNHTiD49904329c7942C7Ac8dC70625041265")
Settings.embed_model = OpenAIEmbedding(api_base="https://ai-yyds.com/v1", api_key="sk-aal58s2gWETYNHTiD49904329c7942C7Ac8dC70625041265")
if "messages" not in st.session_state.keys(): # Initialize the chat messages history
    st.session_state.messages = [
        {"role": "assistant", "content": "å‘æˆ‘æé—®å§!"}
    ]



@st.cache_resource(show_spinner=False)
def load_embed_model(model_path):
    return SentenceTransformer(model_path)
embed_model = load_embed_model("/home/tuxinyu/models/bge-small-zh-v1.5")
class MyEmbeddingFunction(EmbeddingFunction):
    def __call__(self, texts: Documents) -> Embeddings:
        embeddings = [embed_model.encode(text, normalize_embeddings=True) for text in texts]
        return embeddings
@st.cache_resource(show_spinner=False)
def get_img_collection():
    chroma_client = chromadb.PersistentClient(
        path="/data/my_chromadb_vector", settings=chromadb.config.Settings(anonymized_telemetry=False)
    )
    try:
        chroma_client.delete_collection(name= "demo_img")
        print("Deleted collections ")
    except Exception as e:
        print("Deleted collections failed")
        pass
    img_collection = chroma_client.get_or_create_collection(
        name="demo_img",
        embedding_function=MyEmbeddingFunction()
    )
    return img_collection

img_collection = get_img_collection()

def get_related_img(question: str, distance_threshold: float = 0.5):
    result = img_collection.query(
        query_texts=[question],
        n_results=1,
    )
    try:
        document = result['documents'][0][0]
        metadata = result['metadatas'][0][0]
        distance = result['distances'][0][0]
        return metadata['img_path'] if distance < distance_threshold else '', document
    except Exception as e:
        return '', ''




def chat_with_img(image_path, user_prompt):
    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    base64_image = encode_image( image_path )

    client = OpenAI(
            base_url="https://ai-yyds.com/v1",
            api_key="sk-aal58s2gWETYNHTiD49904329c7942C7Ac8dC70625041265")

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpg;base64,{base64_image}"},
                    },
                ],
            }
        ],
    )
    return response.choices[0].message.content



@st.cache_resource(show_spinner=False)
def index_raw_data(file_path):
    with st.spinner(text="æ­£åœ¨è¿›è¡Œæ–‡ä»¶å¤„ç†ï¼Œ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´."):
        llama_reader = pymupdf4llm.LlamaMarkdownReader()
        documents = llama_reader.load_data(file_path)
        # parse imgs

        if not os.path.exists(f"saved_imgs"):
            os.makedirs(f"saved_imgs")
        print('file_path:', file_path)
        md_text = pymupdf4llm.to_markdown(file_path, write_images=True,
                                          image_path=f"saved_imgs")
        image_list = extract_image_context(md_text, 1)
        for image_text_pair in image_list:
            img_path, text = image_text_pair[0], image_text_pair[1]
            print(img_path, text)
            id = deterministic_uuid(img_path) + "-img"
            img_collection.add(
                documents=text,
                metadatas={'img_path': img_path},
                ids=id,
            )
        index = VectorStoreIndex.from_documents(documents)
        return index


#index = load_data()
#index.storage_context.persist("cache/C++è¯­è¨€ç¨‹åºè®¾è®¡_OCR.pdf")


@st.cache_resource(show_spinner=False)
def upload_file_to_local_path(uploaded_file, saved_prefix):
    if uploaded_file is None:
        return ""
    file_name = uploaded_file.name
    suffix = file_name.split(".")[-1]
    uploaded_file_path = f'{saved_prefix}/{file_name}'
    print('uploading: ' + uploaded_file_path)
    with open(uploaded_file_path, 'wb') as f:
        f.write(uploaded_file.getvalue())
    return uploaded_file_path



uploaded_pdf = st.file_uploader("Choose your file", accept_multiple_files=False)
uploaded_image = st.file_uploader("è¯·ä¸Šä¼ å›¾ç‰‡", type=["jpg", "jpeg", "png"])
uploaded_pdf_path = upload_file_to_local_path(uploaded_pdf, "uploaded_data")
uploaded_img_path = upload_file_to_local_path(uploaded_image, "uploaded_img")
if uploaded_pdf_path:
    chat_index = index_raw_data(uploaded_pdf_path)




# if uploaded_file is not None:
#     index = load_index_from_local_storage(to_be_cached_index_path) if not have_new_file else index_raw_data(uploaded_file_path, to_be_cached_index_path)
#     if "chat_engine" not in st.session_state.keys(): # Initialize the chat engine
#         st.session_state.chat_engine = index.as_chat_engine(chat_mode="condense_plus_context", verbose=True)

if prompt := st.chat_input("Your question"):  # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages: # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            if uploaded_img_path and not uploaded_pdf_path:
                # simple MLLM QA
                response = chat_with_img(uploaded_img_path, prompt)
                st.write(response)
                message = {"role": "assistant", "content": response}
                st.session_state.messages.append(message) # Add response to message history
            elif uploaded_pdf_path and not uploaded_img_path:
                # QA with pdf
                query_engine = chat_index.as_query_engine()
                chat_history = []
                chat_engine = CondenseQuestionChatEngine.from_defaults(
                    query_engine=query_engine,
                    condense_question_prompt=custom_prompt,
                    chat_history=chat_history,
                    verbose=True,
                )
                response = chat_engine.chat(prompt, chat_history).response
                st.write(response)
                message = {"role": "assistant", "content": response}
                st.session_state.messages.append(message)
                # add img
                if 'ç¤ºæ„å›¾' in prompt or 'å›¾ç‰‡' in prompt:
                    img_path, _ = get_related_img(prompt, 1)
                else:
                    img_path, _ = get_related_img(prompt)
                if img_path:
                    image = Image.open(img_path)
                    st.write(image)
                    message = {"role": "assistant", "content": image}
                    st.session_state.messages.append(message)
            elif uploaded_pdf_path and uploaded_img_path:

                # search img
                img_text = chat_with_img(uploaded_img_path, "æˆ‘æ­£åœ¨åšä¸€ä¸ªå›¾ç‰‡æœç´¢çš„ä»»åŠ¡ï¼Œå› æ­¤éœ€è¦ä½ å¸®æˆ‘ç”Ÿæˆä½ å¸®æˆ‘ç”Ÿæˆè¿™å¹…å›¾ç‰‡å¯¹åº”çš„æ–‡å­—æè¿°ä¿¡æ¯ï¼Œä»¥ä¾¿æˆ‘æ ¹æ®æ–‡æœ¬ä¿¡æ¯åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ã€‚è¯·ä½ ç›´æŽ¥ç”Ÿæˆè¯¥å›¾ç‰‡çš„æ–‡å­—æè¿°")
                img_path, document = get_related_img(img_text, 1)
                print(img_text, document)
                image = Image.open(img_path)
                # search in knowledge
                query_engine = chat_index.as_query_engine()
                chat_history = []
                chat_engine = CondenseQuestionChatEngine.from_defaults(
                    query_engine=query_engine,
                    condense_question_prompt=custom_prompt,
                    chat_history=chat_history,
                    verbose=True,
                )
                prompt += '''
                ä»¥ä¸‹æ˜¯ç”¨æˆ·æé—®ä¸­çš„â€œå›¾ç‰‡â€çš„ä¿¡æ¯ï¼Œå¯¹ä½ å›žç­”é—®é¢˜ä¹Ÿè®¸æœ‰å¸®åŠ©ï¼š
                '''
                prompt += document
                response = chat_engine.chat(prompt, chat_history).response
                st.write(response)
                message = {"role": "assistant", "content": response}
                st.session_state.messages.append(message)



                st.write(image)
                message = {"role": "assistant", "content": image}
                st.session_state.messages.append(message)


